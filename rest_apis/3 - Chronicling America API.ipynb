{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chronicling America API :\n",
    "\n",
    "In this notebook, we'll get some data out of the Library of Congress's historical newspapers collection called Chronicling America. You can browse this collection online here: https://chroniclingamerica.loc.gov/. Take a look at this search interface and try a few searches.\n",
    "\n",
    "Remember that an HTTP API is just a way of interacting wiht an application by sending requests to URLs that control the application. You can use the Chronicling America API to get back machine readable data in JSON format by just adding `&format=json` to the end of the url on one of your searches. There is more information on how to formulate searches here https://chroniclingamerica.loc.gov/about/api/#search.\n",
    "\n",
    "## Let's get some Data : GET requests to the ChronAm API\n",
    "\n",
    "The point of exposing and API for an application is to make it easier to write programs that interact with that program. For this example, we're working on getting data out of the API so we'll use the HTTP GET method. we'll use the Python programming language and it's requests library to interact with the API. You can run the code in the cells below by hilighting them and pressing: Shift + Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we start by importing the requests library\n",
    "import requests\n",
    "\n",
    "# we'll also import the JSON library to help us read\n",
    "# data as JSON later on.\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we can use the requests library to make HTTP\n",
    "# requests a lot like how we use the browser. The request\n",
    "# below is the equivalent of typing a URL into your web\n",
    "# browser and hitting enter. Let's try a search of the\n",
    "# Chronicling America API:\n",
    "\n",
    "requests.get(\n",
    "    'https://chroniclingamerica.loc.gov/search/pages/results/?proxtext=dogs&format=json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have got back something like `<Response [200]>`. This is an HTTP status code that means success, but we're hoping to get a little more back than just a message like this, so let's improve on our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this example we'll assign the response to a variable\n",
    "# I'll call 'result'. Try modifying the search below to search\n",
    "# for a differnt term.\n",
    "\n",
    "result = requests.get(\n",
    "    'https://chroniclingamerica.loc.gov/search/pages/results/?proxtext=dogs&format=json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, you should get no response back. Instead we saved the response as 'result'. In the cells below lets look at some of the things we can do with a request result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can check the status code\n",
    "result.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can check what URL we asked for\n",
    "result.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can look at the content of the request\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The above is kind of messy so we can also look \n",
    "# at the content as JSON\n",
    "result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Data\n",
    "\n",
    "Ok, now we have a big pile of data, but how do we know what's really in there? Let's use some of the techniques from the last lesson on working with JSON to explore what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First let's assign some data to a variable to make it\n",
    "# easier to work with.\n",
    "\n",
    "data = result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that we can use the .keys() method on a JSON object\n",
    "# try calling it on the data object below \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, let's see whats in some of those keys, remember\n",
    "# that the syntax retrieving the value associate with\n",
    "# a keys is data['keyname']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like the meat of the data is in the items element\n",
    "# I'll assign one item out of the list to a new variable\n",
    "\n",
    "item = data['items'][6]\n",
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a short program that consumes an API\n",
    "\n",
    "Now that we've had a look at our data, let's write an short program to do some analysis. Since the data includes the full OCR'd text for each article we find with our searchers, let's do some analysis on that text. \n",
    "\n",
    "We'll write a short program that searches the newspaper archive for articles published during the Civil War that match a term. We'll count how many articles come up in in Union, Confederate, and Border states. We'll also try to apply a technique called sentiment analysis to (https://en.wikipedia.org/wiki/Sentiment_analysis) to each article we retreive. For this program, we're using a tool called the Afinn word list. The Afinn word list assigns a positive or negative number to a list of common words depending on whether the words are associated with positive or negative sentiment.\n",
    "\n",
    "While our research project wouldn't pass peer review, I hope it demonstrates that getting data in a machine readable form can save lots of busy work, and demonstrate some common types of techniques used to work with data from HTTP APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll need a list of what states were on what side during the war.\n",
    "# There are nuances here, but for this demonstration I'm using the list\n",
    "# from this nps factsheet https://www.nps.gov/civilwar/facts.htm:\n",
    "\n",
    "union_states = [\n",
    "    'Maine', 'New York', 'New Hampshire',\n",
    "    'Vermont', 'Massachusetts', 'Connecticut',\n",
    "    'Rhode Island', 'Pennsylvania', 'New Jersey',\n",
    "    'Ohio', 'Indiana', 'Illinois', 'Kansas',\n",
    "    'Michigan', 'Wisconsin', 'Minnesota', 'Iowa',\n",
    "    'California', 'Nevada', 'Oregon'\n",
    "]\n",
    "\n",
    "confederate_states = [\n",
    "    'Texas', 'Arkansas', 'Louisiana', 'Tennessee',\n",
    "    'Mississippi', 'Alabama', 'Georgia','Florida',\n",
    "    'South Carolina', 'North Carolina', 'Virginia'\n",
    "]\n",
    "\n",
    "border_states = [\n",
    "    'Maryland', 'Delaware', 'West Virginia',\n",
    "    'Kentucky', 'Missouri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paged Results\n",
    "\n",
    "One common issue we run into when working with APIs is that they'll only give you a handful of results at a time. This is especially true of results from search engines like the one we're using. \n",
    "\n",
    "Try doing a search on the web interface, and go to the second page: https://chroniclingamerica.loc.gov/\n",
    "\n",
    "See if you can find any hints in the URL about how the search engine keeps track of what page you're on.\n",
    "\n",
    "We can take advantage of the paging system to make our program repeat itself for every page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computers are good at counting and repeating things. \n",
    "# This is the technicaue we'll use to get several pages\n",
    "# of results.\n",
    "\n",
    "# count to 20 and assign the \n",
    "# results to a variable called number\n",
    "numbers = range(0, 20)\n",
    "\n",
    "# for every number in the range of numbers\n",
    "# print that number\n",
    "\n",
    "for number in numbers:\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do more complicated procedures for every number\n",
    "for number in numbers:\n",
    "    print('https://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "          '?proxtext=dogs&format=json&page='\n",
    "         + str(number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow neat, every one of those is a valid URL for a page of data, and they're all in order!\n",
    "\n",
    "### Our complete program\n",
    "\n",
    "Our complete program will use the above technique to do a series of calculations for several pages of results with one seach term. Let's talk through it and try running it a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Here's our comlete program\n",
    "\n",
    "# set up our sentiment analysis library\n",
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "\n",
    "# We'll get 10 pages of what the LOC deems are\n",
    "# the most relevant results for our search\n",
    "pages = 10\n",
    "\n",
    "# Set our search terms\n",
    "terms = \"Lincoln\"\n",
    "\n",
    "# start our scores at zero\n",
    "\n",
    "union_score = 0.0\n",
    "border_score = 0.0\n",
    "confederate_score = 0.0\n",
    "\n",
    "union_article_count = 0\n",
    "border_article_count = 0\n",
    "confederate_article_count = 0\n",
    "\n",
    "print(\"calculating...\")\n",
    "\n",
    "for page in range(0, pages):\n",
    "    \n",
    "    print(\"fetching result page... {}\".format(str(page)))\n",
    "    \n",
    "    page_json = requests.get(\n",
    "        \"http://chroniclingamerica.loc.gov/search/pages/results/\"\n",
    "        \"?proxtext={}&page={}&rows=20&date1=1861&date2=1865&format=json\"\n",
    "        .format(terms.lower(), str(page))).json()\n",
    "    \n",
    "    for item in page_json['items']:\n",
    "        \n",
    "        try:\n",
    "            sentiment_score = afinn.score(item['ocr_eng'].lower())\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        if item['state'][0] in union_states:\n",
    "            union_score += sentiment_score\n",
    "            union_article_count += 1\n",
    "        elif item['state'][0] in confederate_states:\n",
    "            confederate_score += sentiment_score\n",
    "            confederate_article_count += 1\n",
    "        elif item['state'][0] in border_states:\n",
    "            border_score += sentiment_score\n",
    "            border_article_count += 1\n",
    "\n",
    "# At the end, we'll just print out our results     \n",
    "print(\"\\n\")\n",
    "print(\"union sentiment score = \" + str(union_score))\n",
    "print(\"union total articles = \" + str(union_article_count))\n",
    "print(\"union average afinn score per article = \" + str(union_score / union_article_count))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"confederate sentiment score = \" + str(confederate_score))\n",
    "print(\"confederate total articles = \" + str(confederate_article_count))\n",
    "print(\"confederate average afinn score per article = \" + str(confederate_score / confederate_article_count))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"border sentiment score = \" + str(union_score))\n",
    "print(\"border total articles = \" + str(border_article_count))\n",
    "print(\"border average afinn score per article = \" + str(border_score / border_article_count))\n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
